{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Lab 3: Adversarial Examples Lab\n",
    "\n",
    "### Introduction\n",
    "\n",
    "In this lab, we will explore the concept of adversarial examples as introduced in the seminal paper by Goodfellow et al. (2015): \"Explaining and Harnessing Adversarial Examples.\" Adversarial examples are inputs to machine learning models that have been subtly modified to cause the model to make a mistake, while appearing nearly identical to the original input from a human perspective.\n",
    "\n",
    "### Objectives\n",
    "\n",
    "1. Understand why neural networks are vulnerable to adversarial examples\n",
    "2. Implement the Fast Gradient Sign Method (FGSM) to generate adversarial examples\n",
    "3. Visualize how small, imperceptible perturbations can lead to high-confidence misclassifications\n",
    "\n",
    "### Implementation\n",
    "\n",
    "We'll train a simple deep learning model on the MNIST handwritten digit dataset, then apply FGSM to generate adversarial examples that fool our model.\n",
    "\n",
    "### Setup Instructions\n",
    "\n",
    "This notebook requires PyTorch and related libraries. If you encounter issues installing these dependencies locally, you can run this notebook in Google Colab, which provides these libraries pre-installed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading and Preprocessing\n",
    "\n",
    "In this section, we set up the data pipeline for our experiment:\n",
    "\n",
    "1. **Device Configuration**: We determine whether to use GPU (CUDA) or CPU for computation.\n",
    "\n",
    "2. **Data Transformation**: We apply a simple transformation to convert the MNIST images to PyTorch tensors. Unlike many image classification tasks, we don't normalize the pixel values here because MNIST pixel values are already in the range [0,1] after ToTensor()\n",
    "\n",
    "3. **Dataset Loading**: We load the MNIST dataset, which consists of 28Ã—28 grayscale images of handwritten digits (0-9).\n",
    "   - Training set: 60,000 examples\n",
    "   - Test set: 10,000 examples\n",
    "\n",
    "4. **DataLoader Creation**: We create DataLoader objects that will feed batches of data to our model during training and evaluation.\n",
    "   - We use a batch size of 64\n",
    "   - We shuffle the training data to improve model generalization\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100.0%\n",
      "100.0%\n",
      "100.0%\n",
      "100.0%\n"
     ]
    }
   ],
   "source": [
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.ToTensor(),\n",
    "    ]\n",
    ")\n",
    "\n",
    "batch_size = 64\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=True, transform=transform, download=True\n",
    ")\n",
    "test_dataset = torchvision.datasets.MNIST(\n",
    "    root=\"./data\", train=False, transform=transform, download=True\n",
    ")\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    dataset=train_dataset, batch_size=batch_size, shuffle=True\n",
    ")\n",
    "test_loader = torch.utils.data.DataLoader(\n",
    "    dataset=test_dataset, batch_size=batch_size, shuffle=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [
    {
     "data": {
      "text/plain": "device(type='cpu')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch shape: torch.Size([64, 1, 28, 28])\n",
      "Target shape: torch.Size([64])\n"
     ]
    },
    {
     "data": {
      "text/plain": "<Figure size 800x400 with 2 Axes>",
      "image/png": "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"
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "examples = iter(train_loader)\n",
    "example_data, example_targets = next(examples)\n",
    "\n",
    "print(f\"Batch shape: {example_data.shape}\")  # Should be [batch_size, channels, height, width]\n",
    "print(f\"Target shape: {example_targets.shape}\")  # Should be [batch_size]\n",
    "\n",
    "fig, axes = plt.subplots(1, 2, figsize=(8, 4))\n",
    "axes[0].imshow(example_data[0].squeeze(), cmap='gray')\n",
    "axes[0].set_title(f'Label: {example_targets[0].item()}')\n",
    "axes[1].imshow(example_data[1].squeeze(), cmap='gray')\n",
    "axes[1].set_title(f'Label: {example_targets[1].item()}')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define a simple neural network for MNIST digit classification. \n",
    "\n",
    "Implement a neural network architecture of your choice. For simplicity, we recommend starting with a Multi-Layer Perceptron (MLP) with the following structure:\n",
    "1. An input layer that flattens the 28x28 MNIST images\n",
    "2. A hidden layer with 64 neurons and ReLU activation\n",
    "3. An output layer with 10 neurons (one for each digit 0-9)\n",
    "\n",
    "For implementation, use PyTorch's nn.Module as the base class and implement:\n",
    "- The `__init__` method to define your network layers\n",
    "- The `forward` method to define how data flows through your network\n",
    "\n",
    "For model components, refer to:\n",
    "- [torch.nn.Linear](https://pytorch.org/docs/stable/generated/torch.nn.Linear.html)\n",
    "- [torch.nn.ReLU](https://pytorch.org/docs/stable/generated/torch.nn.ReLU.html)\n",
    "\n",
    "<details>\n",
    "  <summary>Hint 1</summary>\n",
    "  Remember to flatten the input images in the forward method. MNIST images come in shape [batch_size, 1, 28, 28] and need to be reshaped to [batch_size, 784]. Use x = x.view(x.size(0), -1) for this purpose.\n",
    "</details>\n",
    "\n",
    "<details>\n",
    "  <summary>Hint 2</summary>\n",
    "  When creating your first linear layer, the input size should be 784 (28Ã—28Ã—1) to match the flattened MNIST images.\n",
    "</details>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc1 = nn.Linear(28 * 28, 64)\n",
    "        self.fc2 = nn.Linear(64, 10)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.view(x.size(0), -1)\n",
    "        x = F.relu(self.fc1(x))\n",
    "        x = self.fc2(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the Neural Network\n",
    "\n",
    "Below is a basic training loop for our MNIST classifier. We use CrossEntropyLoss as our loss function, which is the standard choice for multi-class classification problems. For optimization, we employ SGD (Stochastic Gradient Descent).\n",
    "\n",
    "The training process consists of:\n",
    "1. Forward pass through the network\n",
    "2. Loss calculation\n",
    "3. Gradient computation via backpropagation\n",
    "4. Parameter updates using the optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 1.2588\n",
      "Epoch [2/10], Loss: 0.5058\n",
      "Epoch [3/10], Loss: 0.3979\n",
      "Epoch [4/10], Loss: 0.3556\n",
      "Epoch [5/10], Loss: 0.3306\n",
      "Epoch [6/10], Loss: 0.3123\n",
      "Epoch [7/10], Loss: 0.2973\n",
      "Epoch [8/10], Loss: 0.2847\n",
      "Epoch [9/10], Loss: 0.2729\n",
      "Epoch [10/10], Loss: 0.2626\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "learning_rate = 1e-2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.SGD(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)           # Step 1: Get predictions\n",
    "        loss = criterion(outputs, labels) # Step 2: Measure error\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()             # Step 3: Clear old gradients\n",
    "        loss.backward()                   # Step 4: Compute new gradients\n",
    "        optimizer.step()                  # Step 5: Update model weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Evaluation\n",
    "We evaluate our model's performance by measuring accuracy on the test dataset. This is a simplified approach - in a real environment, we would typically use a separate validation dataset and employ more comprehensive evaluation metrics. However, for the purposes of this notebook, this straightforward evaluation is sufficient to demonstrate the model's baseline performance before we explore adversarial examples.\n",
    "\n",
    "Weâ€™re testing on â€˜cleanâ€™ images (no tricks yet). A simple MLP should get ~90% accuracy â€” good, but not perfect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on clean test images: 92.82% (expect ~90%)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "clean_acc = 100 * correct / total\n",
    "print(f\"Accuracy on clean test images: {clean_acc:.2f}% (expect ~90%)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing Optimizer Performance\n",
    "\n",
    "For a challenge, try training with Adam, and Adagrad. Plot their losses. Which converges fastest?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.2454\n",
      "Epoch [2/10], Loss: 0.1367\n",
      "Epoch [3/10], Loss: 0.1175\n",
      "Epoch [4/10], Loss: 0.1017\n",
      "Epoch [5/10], Loss: 0.0949\n",
      "Epoch [6/10], Loss: 0.0833\n",
      "Epoch [7/10], Loss: 0.0796\n",
      "Epoch [8/10], Loss: 0.0812\n",
      "Epoch [9/10], Loss: 0.0676\n",
      "Epoch [10/10], Loss: 0.0749\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "learning_rate = 1e-2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)           # Step 1: Get predictions\n",
    "        loss = criterion(outputs, labels) # Step 2: Measure error\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()             # Step 3: Clear old gradients\n",
    "        loss.backward()                   # Step 4: Compute new gradients\n",
    "        optimizer.step()                  # Step 5: Update model weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on clean test images: 96.57% (expect ~90%)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "clean_acc = 100 * correct / total\n",
    "print(f\"Accuracy on clean test images: {clean_acc:.2f}% (expect ~90%)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/10], Loss: 0.3415\n",
      "Epoch [2/10], Loss: 0.2346\n",
      "Epoch [3/10], Loss: 0.2058\n",
      "Epoch [4/10], Loss: 0.1884\n",
      "Epoch [5/10], Loss: 0.1755\n",
      "Epoch [6/10], Loss: 0.1660\n",
      "Epoch [7/10], Loss: 0.1580\n",
      "Epoch [8/10], Loss: 0.1516\n",
      "Epoch [9/10], Loss: 0.1458\n",
      "Epoch [10/10], Loss: 0.1408\n"
     ]
    }
   ],
   "source": [
    "model = Net().to(device)\n",
    "learning_rate = 1e-2\n",
    "\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adagrad(model.parameters(), lr=learning_rate)\n",
    "\n",
    "num_epochs = 10\n",
    "\n",
    "model.train()\n",
    "for epoch in range(num_epochs):\n",
    "    running_loss = 0.0\n",
    "    for images, labels in train_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "\n",
    "        # Forward pass\n",
    "        outputs = model(images)           # Step 1: Get predictions\n",
    "        loss = criterion(outputs, labels) # Step 2: Measure error\n",
    "        # Backward pass and optimize\n",
    "        optimizer.zero_grad()             # Step 3: Clear old gradients\n",
    "        loss.backward()                   # Step 4: Compute new gradients\n",
    "        optimizer.step()                  # Step 5: Update model weights\n",
    "\n",
    "        running_loss += loss.item()\n",
    "\n",
    "    print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {running_loss/len(train_loader):.4f}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on clean test images: 95.72% (expect ~90%)\n"
     ]
    }
   ],
   "source": [
    "model.eval()\n",
    "correct = 0\n",
    "total = 0\n",
    "with torch.no_grad():\n",
    "    for images, labels in test_loader:\n",
    "        images, labels = images.to(device), labels.to(device)\n",
    "        outputs = model(images)\n",
    "        _, predicted = torch.max(outputs.data, 1)\n",
    "        total += labels.size(0)\n",
    "        correct += (predicted == labels).sum().item()\n",
    "\n",
    "clean_acc = 100 * correct / total\n",
    "print(f\"Accuracy on clean test images: {clean_acc:.2f}% (expect ~90%)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### FGSM Attack\n",
    "Implement FGSM attack."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "default_epsilon = 0.15\n",
    "\n",
    "def fgsm_attack(\n",
    "    model: nn.Module, images: torch.Tensor, labels: torch.Tensor, epsilon: float = default_epsilon\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    Creates adversarial examples via the FGSM method:\n",
    "    x_adv = x + epsilon * sign( dJ/dx ).\n",
    "    \"\"\"\n",
    "    images = images.clone().detach().to(device)\n",
    "    labels = labels.to(device)\n",
    "\n",
    "    # Ensure gradients are being tracked\n",
    "    images.requires_grad = True\n",
    "\n",
    "    # TODO: \n",
    "    # 1. Calculate outputs and loss\n",
    "    # 2. Backpropagate loss (use backward method)\n",
    "    # 3. Collect the element-wise sign of the data gradient (you can access it by images.grad.data.sign())\n",
    "    # 4. Create the perturbed image\n",
    "    # 5. Clamp to [0,1] if needed to ensure valid pixel range\n",
    "\n",
    "    outputs = model(images)\n",
    "    loss = criterion(outputs, labels)\n",
    "\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "\n",
    "    ews = images.grad.data.sign()\n",
    "\n",
    "    perturbed_images = images + epsilon * ews\n",
    "\n",
    "    return perturbed_images"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Calculate accuracy for adversarial examples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on adversarial test images (FGSM, eps=0.15): 0.02%\n"
     ]
    }
   ],
   "source": [
    "correct = 0\n",
    "total = 0\n",
    "for images, labels in test_loader:\n",
    "    # Generate adversarial images\n",
    "    adv_images = fgsm_attack(model, images, labels, default_epsilon)\n",
    "\n",
    "    # Re-classify\n",
    "    outputs = model(adv_images.to(device))\n",
    "    _, predicted = torch.max(outputs.data, 1)\n",
    "\n",
    "    total += labels.size(0)\n",
    "    correct += (predicted.cpu() == labels).sum().item()\n",
    "\n",
    "adv_acc = 100 * correct / total\n",
    "print(f\"Accuracy on adversarial test images (FGSM, eps={default_epsilon}): {adv_acc:.2f}%\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Display adversarial examples\n",
    "\n",
    "Let's compare original and adversarial images side-by-side."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "examples = iter(test_loader)\n",
    "example_images, example_labels = next(examples)\n",
    "example_images = example_images[:5]\n",
    "example_labels = example_labels[:5]\n",
    "\n",
    "adv_ex = fgsm_attack(model, example_images, example_labels, default_epsilon)\n",
    "\n",
    "for i in range(5):\n",
    "    plt.figure()\n",
    "\n",
    "    plt.subplot(1, 2, 1)\n",
    "    plt.title(f\"Original - Label: {example_labels[i].item()}\")\n",
    "    plt.imshow(example_images[i][0].cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    adv_label_out = model(adv_ex[i].unsqueeze(0))\n",
    "    _, adv_pred = torch.max(adv_label_out, 1)\n",
    "    plt.subplot(1, 2, 2)\n",
    "    plt.title(f\"Adversarial - Pred: {adv_pred.item()}\")\n",
    "    plt.imshow(adv_ex[i][0].detach().cpu(), cmap=\"gray\")\n",
    "    plt.axis(\"off\")\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Epsilon Parameter Analysis\n",
    " In this section, you'll analyze how the epsilon parameter affects the effectiveness of adversarial examples. The epsilon parameter controls the magnitude of the perturbation applied to the original images.\n",
    "\n",
    "**Task:**\n",
    "1. Evaluate the model's accuracy on adversarial examples generated with different epsilon values (try at least 5 different values)\n",
    "2. Create a line plot showing model accuracy (y-axis) vs. epsilon value (x-axis)\n",
    "3. Discuss the trade-off between:\n",
    "- Visual perceptibility of the perturbations (higher epsilon = more visible changes)\n",
    "- Attack success rate (higher epsilon = more successful attacks = lower model accuracy)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
