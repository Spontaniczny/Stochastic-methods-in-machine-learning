{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Optimization\n",
    "\n",
    "Imagine you need to find the best settings for a complex system – perhaps tuning the hyperparameters of a deep learning model, optimizing the design of an experiment, or finding the ideal parameters for a simulation. Often, evaluating the system with a given set of parameters (i.e., running the experiment or training the model) is expensive, time-consuming, or resource-intensive. How can you find the optimal settings without performing an exhaustive search?\n",
    "\n",
    "This is where **Bayesian Optimization (BO)** comes in. It's a powerful, sequential strategy designed specifically for the **global optimization of black-box functions**. These are functions where:\n",
    "\n",
    "1.  We don't know the underlying mathematical form (it's a \"black box\").\n",
    "2.  Evaluating the function (getting an output `y` for an input `x`) is costly.\n",
    "3.  We often cannot compute derivatives (gradients).\n",
    "4.  The observations might be noisy (the output `y` can vary even for the same input `x`).\n",
    "\n",
    "BO tackles this challenge by intelligently choosing the next point to evaluate, aiming to find the optimum (maximum or minimum) in as few evaluations as possible.\n",
    "\n",
    "### Core Components\n",
    "\n",
    "Bayesian Optimization cleverly balances exploring unknown parts of the search space and exploiting areas known to yield good results. It does this using two main components:\n",
    "\n",
    "1.  **A Probabilistic Surrogate Model:** This model approximates the unknown objective function based on the points observed so far. It also provides uncertainty estimates about the function's behavior in unexplored regions. Gaussian Processes (GPs) are the most common choice for this surrogate model due to their flexibility and inherent uncertainty quantification.\n",
    "2.  **An Acquisition Function:** This function uses the surrogate model's predictions (mean and uncertainty) to determine the 'utility' of evaluating any given point in the search space. It guides the search by suggesting the point that is most likely to lead to an improvement, considering both the predicted performance and the uncertainty. Examples include Expected Improvement (EI) and Upper Confidence Bound (UCB).\n",
    "\n",
    "### The Iterative Process\n",
    "\n",
    "The BO algorithm works iteratively:\n",
    "\n",
    "1.  **Observe:** Evaluate the black-box function at one or more points chosen by the acquisition function.\n",
    "2.  **Update:** Update the surrogate model (e.g., the Gaussian Process) using all the data collected so far (inputs and their observed outputs).\n",
    "3.  **Select:** Optimize the acquisition function (which is cheap to evaluate) over the search space to find the most promising point(s) to evaluate next.\n",
    "4.  **Repeat:** Continue this cycle until a stopping criterion is met (e.g., budget of function evaluations exhausted, desired performance level reached).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended reading:\n",
    "1. [A Tutorial on Bayesian Optimization](https://arxiv.org/pdf/1807.02811)\n",
    "1. [BayesOpt docs]((https://bayesian-optimization.github.io/BayesianOptimization/2.0.3/))\n",
    "1. [Exploring Bayesian Optimization](https://distill.pub/2020/bayesian-optimization/)\n",
    "1. [Acquisition functions in Bayesian Optimization](https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html)\n",
    "1. [A Visual Exploration of Gaussian Processes](https://distill.pub/2019/visual-exploration-gaussian-processes/)\n",
    "1. [Practical Bayesian Optimization of Machine Learning Algorithms](https://arxiv.org/abs/1206.2944)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import gridspec\n",
    "from bayes_opt import BayesianOptimization\n",
    "from bayes_opt import acquisition\n",
    "\n",
    "# Set random seed for reproducibility\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define and visualize the target function\n",
    "First, we'll create a synthetic objective function to optimize. This specific function has three peaks (at x=0, x≈2 and x≈6) and is a toy example.\n",
    "\n",
    "$$f(x) = e^{-(x - 2)^2} + e^{-\\frac{(x - 6)^2}{10}} + \\frac{1}{x^2 + 1}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def target_function(x):\n",
    "    \"\"\"Our example target function to be optimized\"\"\"\n",
    "    return np.exp(-((x - 2) ** 2)) + np.exp(-((x - 6) ** 2) / 10) + 1 / (x**2 + 1)\n",
    "\n",
    "\n",
    "x = np.linspace(-2, 10, 1000)\n",
    "y = target_function(x)\n",
    "\n",
    "plt.figure(figsize=(10, 6))\n",
    "plt.plot(x, y)\n",
    "plt.grid(True)\n",
    "plt.title(\"Target Function\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"f(x)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Define visualization functions\n",
    "These helper functions will visualize the Gaussian Process model\n",
    "and acquisition function at each step of the optimization process.\n",
    "They help us understand how the model evolves as more observations are made."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def posterior(optimizer, grid):\n",
    "    \"\"\"Get posterior mean and standard deviation from GP model\"\"\"\n",
    "    mu, sigma = optimizer._gp.predict(grid, return_std=True)\n",
    "    return mu, sigma\n",
    "\n",
    "def plot_gp(optimizer, x, y, save_path=None):\n",
    "    \"\"\"Plot the Gaussian Process model, observations, and acquisition function\"\"\"\n",
    "    fig = plt.figure(figsize=(16, 10))\n",
    "    steps = len(optimizer.space)\n",
    "    fig.suptitle(\n",
    "        f\"Gaussian Process and Utility Function After {steps} Steps\",\n",
    "        fontsize=30,\n",
    "    )\n",
    "\n",
    "    gs = gridspec.GridSpec(2, 1, height_ratios=[3, 1])\n",
    "    axis = plt.subplot(gs[0])\n",
    "    acq = plt.subplot(gs[1])\n",
    "\n",
    "    x_obs = np.array([[res[\"params\"][\"x\"]] for res in optimizer.res])\n",
    "    y_obs = np.array([res[\"target\"] for res in optimizer.res])\n",
    "\n",
    "    optimizer.acquisition_function._fit_gp(optimizer._gp, optimizer._space)\n",
    "    mu, sigma = posterior(optimizer, x)\n",
    "\n",
    "    axis.plot(x, y, linewidth=3, label=\"Target\")\n",
    "    axis.plot(\n",
    "        x_obs.flatten(), y_obs, \"D\", markersize=8, label=\"Observations\", color=\"r\"\n",
    "    )\n",
    "    axis.plot(x, mu, \"--\", color=\"k\", label=\"Prediction\")\n",
    "\n",
    "    axis.fill(\n",
    "        np.concatenate([x, x[::-1]]),\n",
    "        np.concatenate([mu - 1.9600 * sigma, (mu + 1.9600 * sigma)[::-1]]),\n",
    "        alpha=0.6,\n",
    "        fc=\"c\",\n",
    "        ec=\"None\",\n",
    "        label=\"95% confidence interval\",\n",
    "    )\n",
    "\n",
    "    axis.set_xlim((-2, 10))\n",
    "    axis.set_ylim((None, None))\n",
    "    axis.set_ylabel(\"f(x)\", fontdict={\"size\": 20})\n",
    "    axis.set_xlabel(\"x\", fontdict={\"size\": 20})\n",
    "\n",
    "    utility_function = optimizer.acquisition_function\n",
    "    utility = -1 * utility_function._get_acq(gp=optimizer._gp)(x)\n",
    "    x_flat = x.flatten()\n",
    "\n",
    "    acq.plot(x_flat, utility, label=\"Utility Function\", color=\"purple\")\n",
    "    acq.plot(\n",
    "        x_flat[np.argmax(utility)],\n",
    "        np.max(utility),\n",
    "        \"*\",\n",
    "        markersize=15,\n",
    "        label=\"Next Best Guess\",\n",
    "        markerfacecolor=\"gold\",\n",
    "        markeredgecolor=\"k\",\n",
    "        markeredgewidth=1,\n",
    "    )\n",
    "    acq.set_xlim((-2, 10))\n",
    "    acq.set_ylabel(\"Utility\", fontdict={\"size\": 20})\n",
    "    acq.set_xlabel(\"x\", fontdict={\"size\": 20})\n",
    "\n",
    "    axis.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.0)\n",
    "    acq.legend(loc=2, bbox_to_anchor=(1.01, 1), borderaxespad=0.0)\n",
    "\n",
    "    plt.tight_layout()\n",
    "\n",
    "    if save_path:\n",
    "        plt.savefig(save_path)\n",
    "        plt.close()\n",
    "    else:\n",
    "        plt.show()\n",
    "\n",
    "    return fig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll now see Bayesian Optimization in action, step by step.\n",
    "At each step, observe:\n",
    "1. How the GP model (dashed line) is updated\n",
    "2. How the confidence interval (light blue area) changes\n",
    "3. How the acquisition function (purple line) guides the next sample\n",
    "4. Where the next sample point (gold star) is placed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acquisition_function = acquisition.UpperConfidenceBound(kappa=2.5)\n",
    "optimizer = BayesianOptimization(\n",
    "    f=target_function,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    acquisition_function=acquisition_function,\n",
    "    random_state=27,\n",
    ")\n",
    "\n",
    "# Prepare data for visualization\n",
    "x = np.linspace(-2, 10, 1000).reshape(-1, 1)\n",
    "y = target_function(x)\n",
    "\n",
    "# Initialize with 2 random points\n",
    "optimizer.maximize(init_points=2, n_iter=0)\n",
    "print(\"\\nStep 0: Initial random points\")\n",
    "plot_gp(optimizer, x, y)\n",
    "\n",
    "for i in range(1, 6):\n",
    "    optimizer.maximize(init_points=0, n_iter=1)\n",
    "    print(f\"\\nStep {i}: Added a new observation\")\n",
    "    plot_gp(optimizer, x, y)\n",
    "\n",
    "print(\"\\nFinal result:\")\n",
    "print(f\"Best target value found: {optimizer.max['target']:.4f}\")\n",
    "print(f\"Best input value found: x = {optimizer.max['params']['x']:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Acquisition functions guide the sampling process by balancing:\n",
    "- Exploitation: Sampling where the model predicts high values\n",
    "- Exploration: Sampling where the model is uncertain\n",
    "\n",
    "We'll compare three different acquisition functions:\n",
    "1. Upper Confidence Bound (UCB): Balances exploration and exploitation\n",
    "2. Probability of Improvement (PI): More exploitative\n",
    "3. Greedy: Purely exploitative (custom implementation)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class GreedyAcquisition(acquisition.AcquisitionFunction):\n",
    "    \"\"\"Purely exploitative acquisition function (only considers mean)\"\"\"\n",
    "\n",
    "    def __init__(self, random_state=None):\n",
    "        super().__init__(random_state)\n",
    "\n",
    "    def base_acq(self, mean: np.ndarray, std: np.ndarray) -> np.ndarray:\n",
    "        return mean  # disregard std"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ucb = acquisition.UpperConfidenceBound(kappa=2.5)\n",
    "optimizer_ucb = BayesianOptimization(\n",
    "    f=target_function,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    acquisition_function=ucb,\n",
    "    random_state=27,\n",
    ")\n",
    "\n",
    "optimizer_ucb.maximize(init_points=2, n_iter=5)\n",
    "print(\"\\nUCB Acquisition Function Results:\")\n",
    "plot_gp(optimizer_ucb, x, y)\n",
    "\n",
    "# Run with PI\n",
    "pi = acquisition.ProbabilityOfImprovement(xi=1e-4)\n",
    "optimizer_pi = BayesianOptimization(\n",
    "    f=target_function,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    acquisition_function=pi,\n",
    "    random_state=27,\n",
    ")\n",
    "\n",
    "optimizer_pi.maximize(init_points=2, n_iter=5)\n",
    "print(\"\\nProbability of Improvement Acquisition Function Results:\")\n",
    "plot_gp(optimizer_pi, x, y)\n",
    "\n",
    "# Run with Greedy\n",
    "greedy = GreedyAcquisition()\n",
    "optimizer_greedy = BayesianOptimization(\n",
    "    f=target_function,\n",
    "    pbounds={\"x\": (-2, 10)},\n",
    "    acquisition_function=greedy,\n",
    "    random_state=27,\n",
    ")\n",
    "\n",
    "optimizer_greedy.maximize(init_points=2, n_iter=5)\n",
    "print(\"\\nGreedy (Purely Exploitative) Acquisition Function Results:\")\n",
    "plot_gp(optimizer_greedy, x, y)\n",
    "\n",
    "# Summary\n",
    "print(\"\\nSummary of results:\")\n",
    "print(\n",
    "    f\"UCB best value: {optimizer_ucb.max['target']:.4f} at x = {optimizer_ucb.max['params']['x']:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"PI best value: {optimizer_pi.max['target']:.4f} at x = {optimizer_pi.max['params']['x']:.4f}\"\n",
    ")\n",
    "print(\n",
    "    f\"Greedy best value: {optimizer_greedy.max['target']:.4f} at x = {optimizer_greedy.max['params']['x']:.4f}\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment these results. Write your observations in a Markdown cell."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 1: UCB\n",
    "---------------\n",
    "\n",
    "The parameter kappa in the UCB acquisition function controls the trade-off between:\n",
    "- Exploration (high kappa): More emphasis on the uncertainty\n",
    "- Exploitation (low kappa): More emphasis on the predicted mean\n",
    "\n",
    "Compare results for UCB using 3 different values:\n",
    "1. kappa = 0.1 (Mostly exploitation)\n",
    "2. kappa = 2.5 (Balanced)\n",
    "3. kappa = 5.0 (Mostly exploration)\n",
    "\n",
    "Review this [blog post](https://ekamperi.github.io/machine%20learning/2021/06/11/acquisition-functions.html) and explain the underlying concept of UCB in a Markdown cell."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 2: Chaotic Acquisition Function\n",
    "-----------------------------------------\n",
    "**Goal**: Implement a chaotic acquisition function that exclusively prioritizes the standard deviation of predictions, disregarding the predicted mean. Compare its performance against standard acquisition functions such as UCB and PI (Probability of Improvement)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 3: Expected Improvement\n",
    "--------------------------------\n",
    "\n",
    "**Goal:** Implement the Expected Improvement (EI) acquisition function by extending the `AcquisitionFunction` base class provided by the `BayesOpt` library.\n",
    "\n",
    "Expected Improvement (EI) is a popular choice for the acquisition function. Let's understand its derivation:\n",
    "\n",
    "1.  **Current Best:** After $n$ evaluations, the best function value observed *so far* is $f^* = \\max_{m=1..n} f(x_m)$.\n",
    "\n",
    "2.  **Potential Improvement:** If we evaluate a new point $x$, the *actual* improvement over the current best $f^*$ is $\\max(f(x) - f^*, 0)$, sometimes written as $[f(x) - f^*]^+$.\n",
    "\n",
    "3.  **The Challenge:** We want to choose $x$ to maximize this improvement, but $f(x)$ is unknown *before* evaluation.\n",
    "\n",
    "4.  **The Solution: Expected Improvement:** Our probabilistic model gives a posterior distribution for $f(x)$ at any point $x$. Given the data, this is typically a Gaussian distribution: $f(x) \\sim \\mathcal{N}(\\mu_n(x), \\sigma_n^2(x))$. EI maximizes the *expected* value of the improvement, where the expectation is taken over this posterior distribution:\n",
    "\n",
    "    $EI_n(x) = E_n[ \\max(f(x) - f^*, 0) ]$\n",
    "\n",
    "    This value represents, on average, how much we expect to improve upon $f^*$ by evaluating at $x$. The next point chosen is the one that maximizes $EI_n(x)$.\n",
    "\n",
    "### Mathematical Formulation\n",
    "\n",
    "The expected improvement integral can be computed analytically. The standard closed-form expression is:\n",
    "\n",
    "$$\n",
    "EI(x) =\n",
    "\\begin{cases}\n",
    "(\\mu(x) - f^*) \\Phi(Z) + \\sigma(x) \\varphi(Z) & \\text{if } \\sigma(x) > 0 \\\\\n",
    "0 & \\text{if } \\sigma(x) = 0\n",
    "\\end{cases}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "\n",
    "* $\\mu(x)$: The posterior mean of $f(x)$ at point $x$.\n",
    "* $\\sigma(x)$: The posterior standard deviation of $f(x)$ at point $x$.\n",
    "* $f^*$: The best function value observed so far (a single scalar value).\n",
    "* $Z = \\frac{\\mu(x) - f^*}{\\sigma(x)}$\n",
    "* $\\Phi(Z)$: The Cumulative Distribution Function (CDF) of the standard normal distribution $\\mathcal{N}(0, 1)$.\n",
    "* $\\varphi(Z)$: The Probability Density Function (PDF) of the standard normal distribution $\\mathcal{N}(0, 1)$.\n",
    "\n",
    "### Implementation Task\n",
    "\n",
    "Create a class `ExpectedImprovement` inheriting from `acquisition.AcquisitionFunction` and implement the core calculation in the `base_acq` method. The signature of `base_acq` method looks like this:\n",
    "```python\n",
    "def base_acq(self, mean: NDArray[Float], std: NDArray[Float]) -> NDArray[Float]:\n",
    "```\n",
    "\n",
    "Compare its performance against other acquisition functions.\n",
    "\n",
    "Hint: Analyse the source code of `UpperConfidenceBound` class to better understand how to use/update $f^*$ (`self.y_max`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exercise 4: Theoretical Questions\n",
    "--------------------------------\n",
    "1. Why is Bayesian Optimization particularly useful for expensive-to-evaluate functions?\n",
    "2. Compare and contrast the different acquisition functions discussed in this tutorial.\n",
    "3. What are the limitations of Bayesian Optimization?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
