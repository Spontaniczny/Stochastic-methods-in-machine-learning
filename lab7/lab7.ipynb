{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Normal Distribution\n",
    "\n",
    "This notebook serves as an introduction to univariate and multivariate normal \n",
    "(Gaussian) distributions. Understanding these distributions, particularly the \n",
    "role of the mean and standard deviation/covariance matrix, is fundamental for \n",
    "grasping how certain optimization algorithms, like the Covariance Matrix \n",
    "Adaptation Evolution Strategy (CMA-ES), explore the search space."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Recommended Reading:\n",
    "1. [Are You Normal? Hint: No.](https://allendowney.github.io/ProbablyOverthinkingIt/gaussian.html)\n",
    "2. 3Blue1Brown [But what is the Central Limit Theorem?](https://www.youtube.com/watch?v=zeJD6dqJ5lo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup and Imports\n",
    " \n",
    "First, let's import the necessary libraries and configure the plotting settings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from matplotlib import cm\n",
    "import matplotlib.gridspec as gridspec\n",
    "from mpl_toolkits.axes_grid1 import make_axes_locatable\n",
    "import seaborn as sns\n",
    "import scipy.stats as stats\n",
    "\n",
    "sns.set_style('darkgrid')\n",
    "np.random.seed(42)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. The Univariate Normal (Gaussian) Distribution\n",
    "\n",
    "The simplest case is the one-dimensional, or [univariate, normal distribution](https://en.wikipedia.org/wiki/Normal_distribution). It's perhaps the most common probability distribution.\n",
    "\n",
    "### Definition\n",
    "It's characterized by two parameters:\n",
    "1.  **Mean ($\\mu$)**: This represents the center or expected value of the distribution. It dictates where the peak of the bell curve is located.\n",
    "2.  **Variance ($\\sigma^2$)**: This measures the spread or dispersion of the distribution. A higher variance means the distribution is wider; a lower variance means it's narrower.\n",
    "\n",
    "The **standard deviation ($\\sigma$)** is simply the square root of the variance ($\\sigma = \\sqrt{\\sigma^2}$). It provides a measure of the typical distance of data points from the mean.\n",
    "\n",
    "We denote this distribution as $\\mathcal{N}(\\mu, \\sigma)$.\n",
    "\n",
    "### Probability Density Function (PDF)\n",
    "\n",
    "The shape of the bell curve is defined by the [Probability Density Function (PDF)](https://en.wikipedia.org/wiki/Probability_density_function) (pl. gęstość prawdopodobieństwa). For a given value $x$, the probability density is calculated as:\n",
    "\n",
    "$$p(x \\mid \\mu, \\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} \\exp{ \\left( -\\frac{(x - \\mu)^2}{2\\sigma^2}\\right)}$$\n",
    "\n",
    "The term $\\frac{1}{\\sqrt{2\\pi\\sigma^2}}$ is a normalization constant that ensures the total area under the curve integrates to 1. The `exp(...)` term creates the characteristic bell shape, centered at $\\mu$ and scaled by $\\sigma$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 1: Implement the Univariate PDF\n",
    "\n",
    "Manually implement the PDF function for the univariate normal distribution. Your function should take `x` (a NumPy array, multiple values), `mean`, and `sigma` (standard deviation) as input.\n",
    "\n",
    "Compare the output of your function for a few test cases against the `scipy.stats.norm.pdf`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def univariate_normal_pdf(x: np.ndarray, mean: float, std_dev: float) -> np.ndarray:\n",
    "    raise NotImplementedError(\"Implement `univariate_normal_pdf` function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_x = np.array([-1, 0, 1, 2])\n",
    "test_mean = 0\n",
    "test_sigma = 1\n",
    "\n",
    "pdf_output = univariate_normal_pdf(test_x, mean=test_mean, std_dev=test_sigma)\n",
    "scipy_pdf_output = stats.norm.pdf(test_x, loc=test_mean, scale=test_sigma)\n",
    "\n",
    "print(\"Test x values:\", test_x)\n",
    "print(\"Your PDF output:    \", pdf_output)\n",
    "print(\"Scipy PDF output:   \", scipy_pdf_output)\n",
    "# Check if outputs are close (allowing for floating point differences)\n",
    "print(\"\\nAre outputs close (Your vs Scipy)?\", np.allclose(pdf_output, scipy_pdf_output))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Observe how changing $\\mu$ shifts the curve left or right, and changing $\\sigma$ changes its width and height (while maintaining the total area of 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_vals = np.linspace(-5, 7, num=200)\n",
    "fig = plt.figure(figsize=(7, 4))\n",
    "\n",
    "plt.plot(x_vals, univariate_normal_pdf(x_vals, mean=0, std_dev=1), \n",
    "         label=\"$\\mathcal{N}(\\mu=0, \\sigma=1)$\")\n",
    "plt.plot(x_vals, univariate_normal_pdf(x_vals, mean=2, std_dev=3), \n",
    "         label=\"$\\mathcal{N}(\\mu=2, \\sigma=3)$\")\n",
    "plt.plot(x_vals, univariate_normal_pdf(x_vals, mean=0, std_dev=0.2), \n",
    "         label=\"$\\mathcal{N}(\\mu=0, \\sigma=0.2)$\")\n",
    "\n",
    "plt.xlabel('$x$', fontsize=13)\n",
    "plt.ylabel('Probability Density: $p(x)$', fontsize=13)\n",
    "plt.title('Univariate Normal Distributions')\n",
    "plt.ylim(bottom=0)\n",
    "plt.xlim(x_vals.min(), x_vals.max())\n",
    "plt.legend(loc='best')\n",
    "fig.tight_layout() # Adjust layout\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In a graph of a PDF, `x` is the specific outcome or value along the bottom, and `y` is its associated 'density,' indicating how concentrated the probability is right at that point `x`."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 2: The 3-Sigma Rule\n",
    "\n",
    "Motivation: How to interpret $\\sigma$?\n",
    "\n",
    "The standard deviation ($\\sigma$) gives us a standardized way to understand the spread. The [68-95-99.7 rule](https://en.wikipedia.org/wiki/68%E2%80%9395%E2%80%9399.7_rule), or \"3-sigma rule\", states that for a normal distribution:\n",
    "- Approximately **68%** of the data falls within **1** standard deviation of the mean ($\\mu \\pm 1\\sigma$).\n",
    "- Approximately **95%** of the data falls within **2** standard deviations of the mean ($\\mu \\pm 2\\sigma$).\n",
    "- Approximately **99.7%** of the data falls within **3** standard deviations of the mean ($\\mu \\pm 3\\sigma$).\n",
    "\n",
    "We will verify these rules empirically by sampling.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "- **Generate Samples:** Draw a large number of samples (e.g., N = 10,000) from a standard normal distribution $\\mathcal{N}(0, 1)$ using `np.random.normal`.\n",
    "- **Count Samples in Intervals:** For each interval ($\\mu \\pm 1\\sigma$, $\\mu \\pm 2\\sigma$, $\\mu \\pm 3\\sigma$), count how many of your generated samples fall within that range.\n",
    "- **Calculate Proportions:** Calculate the proportion (or percentage) of samples that fall into each interval (count / total number of samples).\n",
    "- **Compare:** Print your calculated proportions and compare them to the theoretical values (68%, 95%, 99.7%).\n",
    "\n",
    "**Interpretation Questions:**\n",
    "\n",
    "How close are your empirical results to the theoretical percentages?\n",
    "Why might your results not match the theoretical values exactly?\n",
    "(Optional) Try running the code again with a much smaller N (e.g., 100) and a much larger N (e.g., 1,000,000). How does the number of samples affect the accuracy of your empirical verification?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. The Central Limit Theorem (CLT)\n",
    "\n",
    "We've seen the definition and shape of the normal distribution. But *why* is\n",
    "it so important and common in nature and statistics? The Central Limit Theorem\n",
    "provides a powerful answer.\n",
    "\n",
    "### 3.1 Theory: Why is the Normal Distribution Everywhere?\n",
    "\n",
    "The [Central Limit Theorem](https://en.wikipedia.org/wiki/Central_limit_theorem)\n",
    "is a cornerstone of probability theory. In simple terms, it states that if you\n",
    "take the **sum or average** of many independent and identically distributed (i.i.d.)\n",
    "random variables, the distribution of that sum or average will tend towards a\n",
    "**normal distribution**, regardless of the original distribution of the individual\n",
    "variables.\n",
    "\n",
    "Think about it:\n",
    "- Many real-world measurements (like height, weight, errors in experiments) can\n",
    "  be thought of as the result of *many small, independent factors* added together.\n",
    "- In statistics, we often calculate the *average* (mean) of a sample to estimate\n",
    "  a population parameter. The CLT tells us that the distribution of these *sample means*\n",
    "  will be approximately normal, even if the underlying population data isn't,\n",
    "  provided our sample size is large enough.\n",
    "\n",
    "This allows us to use the properties of the normal distribution for statistical\n",
    "inference (like constructing confidence intervals or performing hypothesis tests)\n",
    "even when we don't know the original data's distribution.\n",
    "\n",
    "We can demonstrate this empirically using simulation. Let's simulate a simple\n",
    "process: tossing a coin many times and calculating the proportion of tails (which\n",
    "is the *average* result, if we code Tails=1, Heads=0). We'll repeat this experiment\n",
    "many times (trials) and look at the distribution of these calculated proportions.\n",
    "\n",
    "### 3.2 Simulating Sample Means (Coin Tosses)\n",
    "\n",
    "We'll define a function to run a coin-tossing experiment multiple times. Each\n",
    "experiment (trial) involves tossing a coin a fixed number of times and calculating\n",
    "the proportion of tails. According to the CLT, the distribution of these proportions\n",
    "should look increasingly normal as the number of tosses per trial increases."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_experiment(number_of_trials: int, number_of_tosses: int, probabilities: list[float] = [0.5, 0.5]) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Simulates a coin-tossing experiment across multiple trials and returns the estimated\n",
    "    probability (proportion) of getting tails in each trial.\n",
    "\n",
    "    Parameters:\n",
    "    ----------\n",
    "    number_of_trials : int\n",
    "        The number of independent trials to run. Each trial consists of multiple coin tosses.\n",
    "    number_of_tosses : int\n",
    "        The number of coin tosses to perform in each trial.\n",
    "    probabilities : list[float], optional\n",
    "        A list representing the probabilities for the two outcomes [HEADS (0), TAILS (1)].\n",
    "        By default, this is set to [0.5, 0.5], simulating a fair coin.\n",
    "\n",
    "    Returns:\n",
    "    -------\n",
    "    np.ndarray\n",
    "        A NumPy array of floats, where each element is the proportion of tails (1) observed in\n",
    "        a single trial. The length of the array is equal to `number_of_trials`.\n",
    "\n",
    "    Description:\n",
    "    ------------\n",
    "    For each trial, the function simulates `number_of_tosses` coin flips using the specified\n",
    "    probabilities (0 for Heads, 1 for Tails). It then computes the mean of these flips,\n",
    "    which represents the proportion of tails for that trial. The results from all trials\n",
    "    are collected and returned.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement `run_experiment` function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_clt_histogram(proportions: np.ndarray, number_of_tosses: int) -> None:\n",
    "    plt.figure(figsize=(8, 5))\n",
    "    sns.histplot(proportions, kde=True, stat='density')\n",
    "    plt.title(f\"Distribution of Sample Mean (Proportion of Tails)\\n\"\n",
    "              f\"{len(proportions)} Trials, {number_of_tosses} Tosses/Trial\")\n",
    "    plt.xlabel(\"Proportion of Tails in a Trial (Sample Mean)\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 Exercise 3: Observing the CLT in Action\n",
    "\n",
    "Run the experiment for different `number_of_trials` and `number_of_tosses`. Visualize the results. Use a biased coin, for example, `probabilities = [0.8, 0.2]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. The Multivariate Normal Distribution\n",
    "\n",
    "Now, let's extend this concept to multiple dimensions. The [multivariate normal distribution](https://en.wikipedia.org/wiki/Multivariate_normal_distribution) describes the distribution of a vector of random variables, where each variable is normally distributed and potentially correlated with the others.\n",
    "\n",
    "### Definition\n",
    "For a $d$-dimensional random vector $\\mathbf{x} = [x_1, x_2, ..., x_d]^T$, the multivariate normal distribution is defined by:\n",
    "1.  **Mean Vector ($\\mathbf{\\mu}$)**: A $d$-dimensional vector representing the expected value for each dimension. $\\mathbf{\\mu} = [\\mu_1, \\mu_2, ..., \\mu_d]^T$.\n",
    "2.  **Covariance Matrix ($\\Sigma$)**: A $d \\times d$ symmetric and positive semi-definite matrix.\n",
    "    * The diagonal elements ($\\Sigma_{ii}$) represent the variance of the $i$-th variable ($Var(x_i) = \\sigma_i^2$).\n",
    "    * The off-diagonal elements ($\\Sigma_{ij}$) represent the covariance between the $i$-th and $j$-th variables ($Cov(x_i, x_j)$). Covariance indicates how two variables change together.\n",
    "        * $Cov(x_i, x_j) > 0$: $x_i$ tends to increase when $x_j$ increases (positive correlation).\n",
    "        * $Cov(x_i, x_j) < 0$: $x_i$ tends to decrease when $x_j$ increases (negative correlation).\n",
    "        * $Cov(x_i, x_j) = 0$: $x_i$ and $x_j$ are uncorrelated (if jointly normal, this also means they are independent).\n",
    "\n",
    "We denote this distribution as $\\mathcal{N}(\\mathbf{\\mu}, \\Sigma)$.\n",
    "\n",
    "### Probability Density Function (PDF)\n",
    "\n",
    "The PDF for a $d$-dimensional vector $\\mathbf{x}$ is given by:\n",
    "\n",
    "$$\n",
    "p(\\mathbf{x} \\mid \\mathbf{\\mu}, \\Sigma) = \\frac{1}{\\sqrt{(2\\pi)^d |\\Sigma|}} \\exp{ \\left( -\\frac{1}{2}(\\mathbf{x} - \\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu}) \\right)}\n",
    "$$\n",
    "\n",
    "Where:\n",
    "- $|\\Sigma|$ is the determinant of the covariance matrix.\n",
    "- $\\Sigma^{-1}$ is the inverse of the covariance matrix.\n",
    "- $(\\mathbf{x} - \\mathbf{\\mu})^T \\Sigma^{-1} (\\mathbf{x} - \\mathbf{\\mu})$ is a quadratic form known as the Mahalanobis distance squared, which measures the distance from $\\mathbf{x}$ to the mean $\\mathbf{\\mu}$, accounting for the covariance structure.\n",
    "\n",
    "Let's define a function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def multivariate_normal_pdf(x: np.ndarray, mean: np.ndarray, covariance: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Computes the multivariate normal PDF for a batch of input vectors `x`.\n",
    "\n",
    "    Parameters:\n",
    "        x : np.ndarray Shape (N, D), where N is the number of samples and D is the dimensionality.\n",
    "        mean : np.ndarray Shape (D,), the mean vector.\n",
    "        covariance : np.ndarray Shape (D, D), the covariance matrix.\n",
    "\n",
    "    Returns:\n",
    "        np.ndarray: Shape (N,), the PDF values for each input sample.\n",
    "    \"\"\"\n",
    "    raise NotImplementedError(\"Implement `multivariate_normal_pdf` function.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualization (2D Case)\n",
    "\n",
    "Visualizing beyond 2 or 3 dimensions is difficult. Let's focus on the bivariate ($d=2$) case to understand the impact of the covariance matrix. We'll plot the probability density as contours.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_surface(mean: np.ndarray, covariance: np.ndarray, num_points: int = 50, range_std_dev: int = 3):\n",
    "    \"\"\"\n",
    "    Generates a surface grid of multivariate normal PDF values over a 2D mesh.\n",
    "\n",
    "    Parameters:\n",
    "        mean (np.ndarray): Shape (2,) or (2,1), the mean vector.\n",
    "        covariance (np.ndarray): Shape (2,2), the covariance matrix.\n",
    "        num_points (int): Number of points along each axis.\n",
    "        range_std_dev (int): Range of standard deviations to cover.\n",
    "\n",
    "    Returns:\n",
    "        x1, x2 (np.ndarray): Meshgrid arrays.\n",
    "        pdf (np.ndarray): Computed PDF values at each grid point.\n",
    "        bounds (tuple): (x1_min, x1_max, x2_min, x2_max)\n",
    "    \"\"\"\n",
    "    mean = mean.flatten()\n",
    "    std_devs = np.sqrt(np.diag(covariance))\n",
    "\n",
    "    x1_min = mean[0] - range_std_dev * std_devs[0]\n",
    "    x1_max = mean[0] + range_std_dev * std_devs[0]\n",
    "    x2_min = mean[1] - range_std_dev * std_devs[1]\n",
    "    x2_max = mean[1] + range_std_dev * std_devs[1]\n",
    "\n",
    "    x1s = np.linspace(x1_min, x1_max, num=num_points)\n",
    "    x2s = np.linspace(x2_min, x2_max, num=num_points)\n",
    "    x1, x2 = np.meshgrid(x1s, x2s)\n",
    "\n",
    "    points = np.column_stack([x1.ravel(), x2.ravel()])\n",
    "    pdf_values = multivariate_normal_pdf(points, mean, covariance)\n",
    "    pdf = pdf_values.reshape(num_points, num_points)\n",
    "\n",
    "    return x1, x2, pdf, (x1_min, x1_max, x2_min, x2_max)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define distributions\n",
    "mean1 = np.array([0., 0.])\n",
    "cov1 = np.array([[1., 0.], \n",
    "                 [0., 1.]])\n",
    "\n",
    "mean2 = np.array([0., 1.])\n",
    "cov2 = np.array([[1.0, 0.8], \n",
    "                 [0.8, 1.0]])\n",
    "\n",
    "num_points = 100\n",
    "range_std = 3\n",
    "x1_1, x2_1, pdf1, lim1 = generate_surface(mean1, cov1, num_points, range_std)\n",
    "x1_2, x2_2, pdf2, lim2 = generate_surface(mean2, cov2, num_points, range_std)\n",
    "\n",
    "# Plot setup\n",
    "fig, axes = plt.subplots(1, 2, figsize=(12, 5), constrained_layout=True)\n",
    "levels = 40\n",
    "cmap = cm.viridis\n",
    "\n",
    "# Plot: Independent\n",
    "cf1 = axes[0].contourf(x1_1, x2_1, pdf1, levels=levels, cmap=cmap)\n",
    "axes[0].set_title(\"Independent Variables\", fontsize=13)\n",
    "axes[0].set_xlabel(r'$x_1$', fontsize=12)\n",
    "axes[0].set_ylabel(r'$x_2$', fontsize=12)\n",
    "axes[0].set_aspect('equal')\n",
    "axes[0].axhline(mean1[1], color='gray', ls='--', lw=0.6)\n",
    "axes[0].axvline(mean1[0], color='gray', ls='--', lw=0.6)\n",
    "\n",
    "# Plot: Correlated\n",
    "cf2 = axes[1].contourf(x1_2, x2_2, pdf2, levels=levels, cmap=cmap)\n",
    "axes[1].set_title(\"Positively Correlated\", fontsize=13)\n",
    "axes[1].set_xlabel(r'$x_1$', fontsize=12)\n",
    "axes[1].set_ylabel(r'$x_2$', fontsize=12)\n",
    "axes[1].set_aspect('equal')\n",
    "axes[1].axhline(mean2[1], color='gray', ls='--', lw=0.6)\n",
    "axes[1].axvline(mean2[0], color='gray', ls='--', lw=0.6)\n",
    "\n",
    "cbar = fig.colorbar(cf1, ax=axes, orientation='vertical', shrink=0.85, pad=0.05)\n",
    "cbar.set_label(r'Probability Density $p(x_1, x_2)$', fontsize=12)\n",
    "fig.suptitle('Bivariate Normal Distributions: Independent vs Correlated', fontsize=15, y=1.03)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Observations:**\n",
    "- **Independent Case:** The contours are circular (or elliptical aligned with axes if variances differ). The distribution spreads equally along the axes defined by the standard deviations ($\\sigma_1 = \\sqrt{1}=1, \\sigma_2 = \\sqrt{1}=1$).\n",
    "- **Correlated Case:** The contours are elliptical and tilted. The positive covariance (0.8) means that higher values of $x_1$ are associated with higher values of $x_2$. The main axis of the ellipse reflects this correlation direction. The mean vector shifts the center of the distribution to $(0, 1)$.\n",
    "\n",
    "### Exercise 4: Visualizing Covariance Matrices\n",
    "\n",
    "Explore how different covariance matrices affect the shape and orientation of a 2D normal distribution centered at $\\mathbf{\\mu} = [0, 0]^T$.\n",
    "\n",
    "**Task:** For each of the covariance matrices below, use the `generate_surface` function to plot the corresponding 2D normal distribution. Describe your observations for each case – how does the shape/orientation relate to the values in the covariance matrix?\n",
    "\n",
    "1.  `cov_a = np.array([[2.0, 0.0], [0.0, 0.5]])`\n",
    "2.  `cov_b = np.array([[1.0, -0.7], [-0.7, 1.0]])`\n",
    "3.  `cov_c = np.array([[1.5, 0.9], [0.9, 1.5]])`\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exercise 5*: Eigen-decomposition and Geometric Interpretation\n",
    "\n",
    "**Goal:** To visually confirm how the mathematical components of the covariance matrix ($\\Sigma$) determine the geometric shape of the 2D normal distribution.\n",
    "\n",
    "**Core Idea:**\n",
    "The **eigenvectors** of $\\Sigma$ point along the principal axes (major and minor axes) of the probability density ellipse, defining its **orientation**. The **square root of the eigenvalues** ($\\sqrt{\\lambda_i}$) correspond to the **standard deviation** of the distribution along those principal axes, defining the **spread** or scale along each axis.\n",
    "\n",
    "**Task:**\n",
    "\n",
    "1.  **Choose Parameters:** Select a 2D mean vector $\\mu$ (e.g., `[0, 0]`) and a *non-diagonal* covariance matrix $\\Sigma$ from the previous exercise (e.g., `cov_b` or `cov_c`).\n",
    "2.  **Calculate Decomposition:** Use `np.linalg.eig` to compute the eigenvalues ($\\lambda_i$) and eigenvectors ($V$) of your chosen $\\Sigma$.\n",
    "3.  **Plot Contours:** Use the `generate_surface` function and `plt.contourf` to plot the probability density contours for your chosen $\\mu$ and $\\Sigma$.\n",
    "4.  **Plot Eigenvector Axes:** On the *same plot*:\n",
    "    * For each eigenvector, draw a line starting at the mean $\\mu$.\n",
    "    * The line should point in the eigenvector's direction.\n",
    "    * The length of the line should represent the spread along that axis. A good choice is to make the total length $2 \\times \\text{scale\\_factor} \\times \\sqrt{\\lambda_i}$ (e.g., `scale_factor=2` for $\\pm 2$ standard deviations), centered at the mean. Use `plt.plot` or `ax.quiver`.\n",
    "5.  **Interpret:** Observe your plot. Do the lines you drew (representing the scaled eigenvectors) align perfectly with the ellipse's axes? Does the length of each line correspond to the spread (width/height) of the ellipse along that direction?\n",
    "\n",
    "*(Remember to use `ax.set_aspect('equal')` for accurate geometric representation!)*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
